#!/usr/bin/env python3
"""
ğŸš€ Algoritmos Visualizador - Multi-Platform Launcher
====================================================

Launcher inteligente que detecta automaticamente o ambiente de execuÃ§Ã£o
e configura a aplicaÃ§Ã£o adequadamente para diferentes plataformas.

Plataformas Suportadas:
- ğŸ–¥ï¸  Local Development (VS Code, terminal)
- ğŸŒ Google Colab (GPU/TPU support)
- ğŸ¤— Hugging Face Spaces (GPU persistent)
- â˜ï¸  Streamlit Cloud (CPU-only optimized)

Funcionalidades:
- âœ… Auto-detecÃ§Ã£o de ambiente
- âœ… ConfiguraÃ§Ã£o automÃ¡tica de dependÃªncias
- âœ… OtimizaÃ§Ã£o de recursos por plataforma
- âœ… Logs detalhados de inicializaÃ§Ã£o
"""

import os
import sys
import platform
import subprocess
from pathlib import Path
import importlib.util
from typing import Dict, Any, List

class PlatformDetector:
    """Detector inteligente de plataformas de execuÃ§Ã£o."""

    def __init__(self):
        self.platform_info = self._detect_platform()
        self.gpu_info = self._detect_gpu()
        self.dependencies_status = {}

    def _detect_platform(self) -> Dict[str, Any]:
        """Detecta a plataforma atual."""
        info = {
            'name': 'unknown',
            'is_colab': False,
            'is_huggingface': False,
            'is_streamlit_cloud': False,
            'is_local': True,
            'python_version': platform.python_version(),
            'system': platform.system().lower()
        }

        # Detect Google Colab
        if 'COLAB_GPU' in os.environ or 'COLAB_TPU_ADDR' in os.environ:
            info.update({
                'name': 'google_colab',
                'is_colab': True,
                'is_local': False
            })

        # Detect Hugging Face Spaces
        if 'SPACE_ID' in os.environ or 'HF_SPACE' in os.environ:
            info.update({
                'name': 'huggingface_spaces',
                'is_huggingface': True,
                'is_local': False
            })

        # Detect Streamlit Cloud
        if 'STREAMLIT_SERVER_HEADLESS' in os.environ:
            info.update({
                'name': 'streamlit_cloud',
                'is_streamlit_cloud': True,
                'is_local': False
            })

        return info

    def _detect_gpu(self) -> Dict[str, Any]:
        """Detecta informaÃ§Ãµes da GPU."""
        gpu_info = {
            'available': False,
            'type': 'cpu',
            'name': 'CPU',
            'memory_gb': 0
        }

        try:
            import torch
            gpu_info['available'] = torch.cuda.is_available()
            if gpu_info['available']:
                gpu_info.update({
                    'type': 'cuda',
                    'name': torch.cuda.get_device_name(0),
                    'memory_gb': torch.cuda.get_device_properties(0).total_memory / (1024**3)
                })
        except ImportError:
            pass

        return gpu_info

    def check_dependencies(self) -> Dict[str, bool]:
        """Verifica status das dependÃªncias crÃ­ticas."""
        dependencies = {
            'streamlit': 'streamlit',
            'torch': 'torch',
            'numpy': 'numpy',
            'pandas': 'pandas',
            'matplotlib': 'matplotlib',
            'plotly': 'plotly'
        }

        status = {}
        for name, module in dependencies.items():
            try:
                importlib.import_module(module)
                status[name] = True
            except ImportError:
                status[name] = False

        self.dependencies_status = status
        return status

    def get_optimization_config(self) -> Dict[str, Any]:
        """Retorna configuraÃ§Ã£o otimizada para a plataforma."""
        base_config = {
            'cache_enabled': True,
            'gpu_acceleration': self.gpu_info['available'],
            'memory_limit': None,
            'timeout_limit': None,
            'debug_mode': False
        }

        if self.platform_info['is_colab']:
            base_config.update({
                'platform_name': 'Google Colab',
                'memory_limit': 'ilimitado',
                'timeout_limit': 'ilimitado',
                'gpu_acceleration': True,
                'recommended_features': ['redes_neurais', 'arte_generativa', 'competicoes']
            })

        elif self.platform_info['is_huggingface']:
            base_config.update({
                'platform_name': 'Hugging Face Spaces',
                'memory_limit': '16GB',
                'timeout_limit': 'ilimitado',
                'gpu_acceleration': True,
                'recommended_features': ['modelos_ml', 'api_integrations', 'deploy_profissional']
            })

        elif self.platform_info['is_streamlit_cloud']:
            base_config.update({
                'platform_name': 'Streamlit Cloud',
                'memory_limit': '1GB',
                'timeout_limit': '10-15min',
                'gpu_acceleration': False,
                'recommended_features': ['visualizacoes', 'exercicios', 'aprendizado_basico']
            })

        else:
            base_config.update({
                'platform_name': 'Local Development',
                'memory_limit': 'sistema',
                'timeout_limit': 'ilimitado',
                'gpu_acceleration': self.gpu_info['available'],
                'recommended_features': ['desenvolvimento', 'debugging', 'testing']
            })

        return base_config

class MultiPlatformLauncher:
    """Launcher multi-plataforma inteligente."""

    def __init__(self):
        self.detector = PlatformDetector()
        self.config = self.detector.get_optimization_config()

    def install_dependencies(self) -> bool:
        """Instala dependÃªncias especÃ­ficas da plataforma."""
        print("ğŸ“¦ Verificando/instalando dependÃªncias...")

        platform_deps = {
            'google_colab': [
                'torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118',
                'streamlit numpy pandas matplotlib plotly seaborn scipy',
                'requests pillow tavily-python psutil'
            ],
            'huggingface_spaces': [
                'streamlit torch torchvision torchaudio numpy pandas matplotlib plotly',
                'seaborn scipy requests pillow tavily-python psutil'
            ],
            'streamlit_cloud': [
                'streamlit torch==2.0.0+cpu torchvision==0.15.0+cpu torchaudio==2.0.0',
                'numpy pandas matplotlib plotly seaborn scipy',
                'requests pillow tavily-python psutil'
            ]
        }

        platform_name = self.detector.platform_info['name']
        if platform_name in platform_deps:
            deps = platform_deps[platform_name]
            for dep in deps:
                try:
                    cmd = f"pip install -q {dep}"
                    result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=300)
                    if result.returncode != 0:
                        print(f"   âš ï¸  Erro instalando {dep}: {result.stderr}")
                        return False
                except Exception as e:
                    print(f"   âŒ Erro: {e}")
                    return False

        print("   âœ… DependÃªncias OK")
        return True

    def show_platform_info(self):
        """Exibe informaÃ§Ãµes detalhadas da plataforma."""
        print("ğŸ–¥ï¸  DETECÃ‡ÃƒO DE PLATAFORMA")
        print("=" * 50)

        info = self.detector.platform_info
        gpu = self.detector.gpu_info

        print(f"ğŸ“ Plataforma: {self.config['platform_name']}")
        print(f"ğŸ Python: {info['python_version']}")
        print(f"ğŸ’» Sistema: {info['system']}")

        if gpu['available']:
            print(f"ğŸ® GPU: {gpu['name']} ({gpu['memory_gb']:.1f}GB)")
        else:
            print("âš¡ GPU: NÃ£o disponÃ­vel (CPU-only)")

        print(f"ğŸ§  MemÃ³ria limite: {self.config['memory_limit']}")
        print(f"â±ï¸  Timeout: {self.config['timeout_limit']}")

        print("\nğŸ¯ Recursos recomendados:")
        for feature in self.config['recommended_features']:
            print(f"   â€¢ {feature.replace('_', ' ').title()}")

    def launch_application(self, app_file: str = "app_integrada.py", port: int = 8501):
        """LanÃ§a a aplicaÃ§Ã£o com configuraÃ§Ãµes otimizadas."""
        print("\nğŸš€ INICIANDO APLICAÃ‡ÃƒO")
        print("=" * 50)

        # Configurar variÃ¡veis de ambiente
        env_vars = {
            'PLATFORM_DETECTED': self.detector.platform_info['name'],
            'GPU_AVAILABLE': str(self.detector.gpu_info['available']).lower(),
            'CACHE_ENABLED': str(self.config['cache_enabled']).lower(),
        }

        if self.detector.platform_info['is_colab']:
            env_vars.update({
                'STREAMLIT_SERVER_HEADLESS': 'true',
                'STREAMLIT_SERVER_PORT': str(port),
                'STREAMLIT_SERVER_ADDRESS': '0.0.0.0'
            })

        # Construir comando
        cmd_parts = ["streamlit", "run", app_file]

        # Adicionar configuraÃ§Ãµes especÃ­ficas
        if self.detector.platform_info['is_colab']:
            cmd_parts.extend(["--server.port", str(port), "--server.address", "0.0.0.0"])
        elif self.detector.platform_info['is_huggingface']:
            cmd_parts.extend(["--server.port", str(port), "--server.address", "0.0.0.0"])
        else:
            cmd_parts.extend(["--server.headless", "true", "--server.port", str(port)])

        cmd = " ".join(cmd_parts)

        print(f"ğŸ¯ Executando: {cmd}")
        print(f"ğŸŒ URL esperada: http://localhost:{port}")
        if self.detector.platform_info['is_colab']:
            print("ğŸ”— No Colab, um link externo serÃ¡ gerado automaticamente")

        print("\n" + "=" * 50)

        # Executar
        try:
            os.environ.update(env_vars)
            subprocess.run(cmd, shell=True)
        except KeyboardInterrupt:
            print("\nâ¹ï¸  AplicaÃ§Ã£o interrompida pelo usuÃ¡rio")
        except Exception as e:
            print(f"\nâŒ Erro ao executar aplicaÃ§Ã£o: {e}")

def main():
    """FunÃ§Ã£o principal."""
    print("ğŸ¯ ALGORITMOS VISUALIZADOR - MULTI-PLATFORM LAUNCHER")
    print("=" * 60)

    launcher = MultiPlatformLauncher()

    # Detectar plataforma
    launcher.show_platform_info()

    # Verificar dependÃªncias
    print("\nğŸ“¦ Verificando dependÃªncias...")
    deps_status = launcher.detector.check_dependencies()

    missing_deps = [name for name, status in deps_status.items() if not status]
    if missing_deps:
        print(f"   âš ï¸  DependÃªncias faltando: {', '.join(missing_deps)}")
        if launcher.detector.platform_info['is_colab'] or launcher.detector.platform_info['is_huggingface']:
            print("   ğŸ”„ Instalando dependÃªncias automaticamente...")
            if not launcher.install_dependencies():
                print("   âŒ Falha na instalaÃ§Ã£o. Saindo...")
                return 1
        else:
            print("   ğŸ’¡ Execute: pip install -r requirements.txt")
            return 1
    else:
        print("   âœ… Todas as dependÃªncias OK")

    # Verificar arquivo da aplicaÃ§Ã£o
    app_file = "app_integrada.py"
    if not Path(app_file).exists():
        print(f"   âŒ Arquivo {app_file} nÃ£o encontrado")
        return 1

    print("   âœ… Arquivo da aplicaÃ§Ã£o OK")

    # Launch application
    launcher.launch_application(app_file)

    return 0

if __name__ == "__main__":
    sys.exit(main())